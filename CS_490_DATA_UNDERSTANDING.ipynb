{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nrd8QuzPVg7",
    "outputId": "1f533d7d-42db-49d8-aa2b-f649c9be95ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\JimHentai\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27488\\1316715972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# check gpu connection\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILNWQWkNqyC3",
    "outputId": "933f4c87-7d85-4f5b-fe93-3c01c55b9e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# connect to drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "_RD35TND-w72",
    "outputId": "9a33dd99-cbfb-420c-ba46-f87389a36e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.6.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.0 MB/s \n",
      "\u001b[?25hCollecting pdfminer.six==20211012\n",
      "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 9.3 MB/s \n",
      "\u001b[?25hCollecting Pillow>=8.4\n",
      "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 24.2 MB/s \n",
      "\u001b[?25hCollecting Wand>=0.6.7\n",
      "  Downloading Wand-0.6.7-py2.py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 48.6 MB/s \n",
      "\u001b[?25hCollecting cryptography\n",
      "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 33.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20211012->pdfplumber) (3.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber) (2.21)\n",
      "Building wheels for collected packages: pdfplumber\n",
      "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pdfplumber: filename=pdfplumber-0.6.0-py3-none-any.whl size=33688 sha256=bcb14cf893236fc972506ae42f038ddd6f0240ffa3f7b3d0e31bcacdfa1b8450\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/56/fe/2e93d842ffa9ea97746c1ab253d43502ed61c0689361a0224e\n",
      "Successfully built pdfplumber\n",
      "Installing collected packages: cryptography, Wand, Pillow, pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed Pillow-9.0.1 Wand-0.6.7 cryptography-36.0.2 pdfminer.six-20211012 pdfplumber-0.6.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taOPj88GSfho"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "9-7k0HMvqyqa",
    "outputId": "fa104477-49fc-4297-c6c0-e09202622088"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-587a7ba7574e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourt_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-587a7ba7574e>\u001b[0m in \u001b[0;36mget_filepaths\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Walk the tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Join the two strings in order to form the full filepath.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/os.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Note that scandir is global in this module due\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# to earlier import-*.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mscandir_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monerror\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This block converts pdf's into pandas dataframe as a string\n",
    "start = time.time()\n",
    "\n",
    "def get_filepaths(directory):\n",
    "  \n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory \n",
    "    tree by walking the tree either top-down or bottom-up. For each \n",
    "    directory in the tree rooted at directory top (including top itself), \n",
    "    it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  # Add it to the list.\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "\n",
    "# run with gpu\n",
    "with tf.device('/device:GPU:0'):  \n",
    "\n",
    "  # court that we want to convert to csv\n",
    "  court_nums = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "  files = get_filepaths(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(court_num))\n",
    "\n",
    "\n",
    "  # randomly select a subset of files\n",
    "  files = np.random.choice(files, 1000, replace= False)\n",
    "\n",
    "  # Find case_id\n",
    "  def find_case_id(file_index):\n",
    "    name = (files[count].split('/')[6])\n",
    "    case_id_temp = name.split('-')\n",
    "    case_id = case_id_temp[2] + \"-\" + case_id_temp[3]\n",
    "    case_problem_identifier = case_id_temp[4][0]\n",
    "\n",
    "    return case_id,case_problem_identifier\n",
    "\n",
    "  # Loop through all pages in pdf file\n",
    "  # **ADDING Extracting the judges name from the first page\n",
    "  def loop(file):\n",
    "    with tf.device('/device:GPU:0'):  \n",
    "      text = \"\"\n",
    "      for page in range(len(file.pages)):\n",
    "              text += file.pages[page].extract_text()\n",
    "\n",
    "      #For court 11 only\n",
    "      #regDOTALL = re.compile(r'\\bBefore\\b.*\\bJudges\\b\\.', re.DOTALL)\n",
    "      #judge_line = regDOTALL.findall(text)\n",
    "      #judges_list = re.findall(r'(\\b[A-Z]+\\b\\s\\b[A-Z]+\\b|\\b[A-Z]+\\b)', judge_line[0])\n",
    "      # Not sure how we want to store judges, each in own column?, as a list?,\n",
    "      # just as another string with all of them concatenated?\n",
    "\n",
    "      return text    #, judges_list - took out\n",
    "\n",
    "  # Make new function for judges?\n",
    "  # For court 11, format is: Before ED CARNES, Chief Judge, MARTIN, and ROSENBAUM, Circuit Judges.\n",
    "  # So usually: Before JUDGE_FIRST JUDGE_LAST, Chief Judge, JUDGE2_LAST, and JUDGE3_LAST, Circuit Judges.\n",
    "  # Always starts with \"Before\", sometimes no \"and\", names always in caps, always separated\n",
    "  # by commas, and typically ends with \"Judge(s).\"\n",
    "  \n",
    "\n",
    "  data = pd.DataFrame(columns=['Court_id','Case_id',\"Opinion\"])   #Added judges here - took out\n",
    "\n",
    "  count = 0\n",
    "  completed_count = 0\n",
    "\n",
    "  # loop through all files in directory\n",
    "  for file in files:\n",
    "    if completed_count == 500:\n",
    "        break;\n",
    "    with tf.device('/device:GPU:0'):  \n",
    "      with pdfplumber.open(file) as f:\n",
    "          case_id, case_problem_id = find_case_id(count)\n",
    "          if len(f.pages) < 2 or case_problem_id == 1:\n",
    "            count+=1\n",
    "            print(count)\n",
    "            continue\n",
    "          all_text = loop(f)\n",
    "          data.loc[len(data.index)] = [court_num, case_id ,all_text]  # Added judges column - took out\n",
    "      completed_count +=1\n",
    "      count+=1\n",
    "      # keep track of where we are\n",
    "      print(count)\n",
    "      data.to_csv(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(court_num)+\".csv\")\n",
    "  \n",
    "  # Convert data to csv\n",
    "  data.to_csv(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(court_num)+\".csv\")\n",
    "\n",
    "\n",
    "  end = time.time()\n",
    "  print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhBrXU-qI_Y6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHg_HOblBOj8"
   },
   "outputs": [],
   "source": [
    "# Code to get number of pages/files\n",
    "\n",
    "from pdfplumber import pdf\n",
    "def get_filepaths(directory):\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            file_paths.append(filepath)  # Add it to the list.\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "\n",
    "def number_of_pages(file):\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    with pdfplumber.open(file) as f:\n",
    "        return len(f.pages)\n",
    "\n",
    "# list all court that you want included\n",
    "court_num = [2]\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  for num in court_num:\n",
    "    sum = 0\n",
    "    lenOne_pages = 0\n",
    "    file_paths = get_filepaths(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(num))\n",
    "    for file in file_paths:\n",
    "      num_pages = number_of_pages(file)\n",
    "      if num_pages == 1:\n",
    "        lenOne_pages += 1\n",
    "      sum+= num_pages\n",
    "    print(\"COURT_\"+str(num) + \":\" + str(len(file_paths)) + \" Total Opionions\")\n",
    "    print(\"COURT_\"+str(num) + \":\" + str(sum) + \" Pages\")\n",
    "    print(\"COURT_\"+str(num) + \":\" + str(lenOne_pages) + \" Pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj8LamPf-eNZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "court_nums = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for courts in court_nums:\n",
    "  data = pd.read_csv(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(courts)+\".csv\")\n",
    "  # data = data.sample(n=500)\n",
    "  # data.to_csv(\"/content/gdrive/MyDrive/CS_490_DATA/Court_\"+str(courts)+\".csv\")\n",
    "  print(len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CJhdvXRlv2j",
    "outputId": "c0215e70-9314-473e-c4fe-47ceec2d8cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"/content/gdrive/MyDrive/CS_490_DATA/Court_8.csv\")\n",
    "\n",
    "text = \" \".join(review for review in data[\"Opinion\"])\n",
    "\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZMc6_mul_Qy"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "with tf.device('/device:GPU:0'):  \n",
    "  text_tokens = word_tokenize(text)\n",
    "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words(\"english\")]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XveJ7h3nPaN",
    "outputId": "92a0137e-b60e-4ab3-8eb1-42876ff3c70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('v', 4498), ('court', 3312), ('The', 3236), ('States', 3172), ('United', 3167), ('F3d', 2644), ('district', 2625), ('Filed', 2604), ('Appellate', 2379), ('Case', 2367), ('Cir', 2356), ('Date', 2249), ('Page', 2233), ('Entry', 2231), ('ID', 2231), ('8th', 2095), ('District', 1717), ('See', 1619), ('Court', 1543), ('____________', 1483), ('Mr', 1345), ('2', 1221), ('evidence', 1186), ('Circuit', 1174), ('In', 1038), ('We', 1035), ('judgment', 1028), ('also', 1027), ('US', 1013), ('sentence', 978), ('appeal', 976), ('would', 929), ('___________________________', 919), ('1', 888), ('review', 864), ('claim', 816), ('law', 805), ('courts', 803), ('No', 792), ('case', 768), ('USC', 748), ('must', 732), ('et', 726), ('al', 709), ('whether', 700), ('Id', 695), ('Eighth', 687), ('Missouri', 680), ('may', 664), ('For', 662), ('3', 662), ('motion', 648), ('A', 634), ('2020', 628), ('opinion', 621), ('Appeals', 608), ('Judge', 605), ('claims', 601), ('see', 600), ('record', 592), ('within', 572), ('time', 572), ('filed', 570), ('one', 569), ('reasonable', 564), ('Ms', 562), ('found', 554), ('petitions', 545), ('two', 527), ('Inc', 521), ('rehearing', 516), ('offense', 495), ('2021', 480), ('state', 475), ('discretion', 466), ('4', 465), ('Michael', 464), ('defendant', 461), ('could', 456), ('Iowa', 452), ('banc', 438), ('sentencing', 421), ('conclude', 420), ('Appellant', 419), ('Before', 419), ('error', 415), ('factors', 413), ('decision', 405), ('received', 404), ('en', 404), ('argues', 397), ('Judges', 395), ('18', 393), ('drug', 392), ('Appellee', 388), ('trial', 385), ('affirm', 385), ('jury', 384), ('5', 384), ('Company', 383), ('bankruptcy', 383), ('based', 381), ('denied', 380), ('petition', 378), ('E', 377), ('Submitted', 371), ('period', 370), ('waiver', 368), ('2019', 366), ('officers', 364), ('agreement', 362), ('conduct', 357), ('order', 356), ('______________________________', 355), ('contract', 351), ('Honorable', 350), ('counsel', 350), ('filing', 347), ('issue', 346), ('14', 345), ('government', 344), ('summary', 344), ('abuse', 343), ('3M', 341), ('I', 337), ('parties', 337), ('finding', 335), ('defendants', 334), ('Appeal', 333), ('8', 331), ('plea', 331), ('omitted', 328), ('2018', 327), ('damages', 327), ('7', 323), ('months', 321), ('date', 321), ('action', 320), ('grant', 318), ('quoting', 317), ('failed', 316), ('required', 316), ('entered', 312), ('made', 311), ('relevant', 310), ('This', 309), ('6', 309), ('America', 308), ('1The', 307), ('Ct', 306), ('fact', 306), ('As', 306), ('testimony', 304), ('standard', 304), ('guilty', 301), ('City', 300), ('Mo', 300), ('first', 299), ('S', 287), ('alleged', 286), ('federal', 285), ('violation', 282), ('Guidelines', 282), ('Act', 281), ('R', 279), ('Clerk', 279), ('lllllllllllllllllllllDefendant', 278), ('issues', 278), ('appeals', 277), ('withdraw', 277), ('even', 277), ('firearm', 277), ('public', 275), ('without', 274), ('clear', 274), ('St', 274), ('Co', 273), ('de', 271), ('314', 271), ('support', 267), ('find', 265), ('right', 265), ('Rules', 265), ('novo', 264), ('information', 262), ('But', 262), ('F', 262), ('argument', 260), ('11', 258), ('lllllllllllllllllllllPlaintiff', 257), ('Gans', 256), ('cause', 256), ('Please', 255), ('release', 255), ('facts', 254), ('Western', 254), ('Street', 254), ('range', 253), ('He', 252), ('court1', 251), ('show', 251), ('It', 251), ('search', 251), ('PER', 250), ('CURIAM', 250), ('On', 249), ('Louis', 249), ('Unpublished', 247), ('testified', 244), ('cases', 244), ('including', 243), ('2017', 243), ('person', 242), ('criminal', 241), ('consider', 239), ('three', 239), ('statute', 237), ('days', 237), ('imposed', 236), ('Minnesota', 234), ('Federal', 234), ('2016', 233), ('brief', 233), ('relief', 231), ('part', 230), ('State', 230), ('use', 229), ('Accordingly', 229), ('First', 229), ('unreasonable', 229), ('Arkansas', 229), ('used', 228), ('day', 227), ('methamphetamine', 224), ('erred', 223), ('9', 223), ('officer', 222), ('After', 221), ('considered', 220), ('clearly', 220), ('question', 219), ('2009', 219), ('2014', 217), ('reviewed', 217), ('M', 217), ('fees', 215), ('10', 214), ('complaint', 214), ('circumstances', 213), ('factual', 213), ('legal', 212), ('interest', 212), ('matter', 210), ('conviction', 208), ('allowed', 208), ('work', 208), ('possession', 206), ('Amendment', 206), ('sufficient', 205), ('party', 203), ('SW3d', 201), ('denial', 199), ('substantial', 199), ('imprisonment', 198), ('new', 198), ('2015', 198), ('LLC', 198), ('D', 197), ('due', 196), ('held', 195), ('irrelevant', 195), ('Williams', 193), ('award', 193), ('moved', 193), ('amount', 192), ('At', 192), ('policy', 192), ('result', 191), ('basis', 190), ('breach', 190), ('issued', 189), ('supervised', 189), ('Debtors', 189), ('Thomas', 188), ('prior', 188), ('years', 187), ('another', 185), ('13', 185), ('Procedure', 184), ('Rule', 183), ('hold', 183), ('term', 182), ('provided', 181), ('When', 181), ('conspiracy', 180), ('Johnson', 180), ('California', 180), ('plan', 180), ('concluded', 179), ('And', 179), ('hearing', 179), ('jurisdiction', 179), ('plaintiffs', 179), ('establish', 178), ('Because', 178), ('South', 178), ('Supreme', 177), ('drivers', 174), ('provide', 174), ('today', 174), ('statutory', 173), ('stated', 172), ('id', 172), ('entitled', 172), ('different', 171), ('findings', 170), ('12', 169), ('rules', 168), ('L', 168), ('request', 167), ('USSG', 167), ('removal', 167), ('factor', 167), ('rights', 167), ('drugs', 166), ('released', 166), ('purpose', 166), ('Officer', 166), ('CRST', 166), ('To', 165), ('timely', 165), ('J', 165), ('2012', 164), ('NW2d', 164), ('determination', 164), ('permitted', 164), ('2011', 163), ('notice', 163), ('particularly', 163), ('need', 162), ('children', 162), ('compliance', 162), ('Eastern', 162), ('prison', 162), ('police', 162), ('entry', 160), ('1000', 160), ('conditions', 160), ('child', 159), ('2013', 159), ('appropriate', 159), ('15', 158), ('Chief', 158), ('level', 158), ('intent', 158), ('fee', 158), ('2010', 157), ('scope', 157), ('10th', 157), ('21', 156), ('proceedings', 156), ('28', 156), ('III', 155), ('February', 155), ('second', 155), ('Southern', 155), ('false', 154), ('enhancement', 154), ('section', 154), ('presented', 154), ('40', 154), ('provision', 154), ('upon', 153), ('App', 153), ('April', 152), ('light', 152), ('citation', 152), ('individual', 152), ('pleaded', 152), ('properly', 152), ('granted', 152), ('John', 151), ('Any', 151), ('Counsel', 150), ('ensure', 149), ('distribute', 148), ('home', 148), ('later', 147), ('explained', 147), ('sentenced', 147), ('procedure', 147), ('address', 147), ('known', 146), ('committed', 146), ('reasons', 146), ('however', 146), ('said', 145), ('office', 145), ('3553a', 145), ('terms', 145), ('2008', 144), ('particular', 144), ('Judgment', 144), ('determined', 144), ('process', 144), ('material', 144), ('trust', 144), ('accordance', 143), ('B', 143), ('established', 143), ('Ackerman', 143), ('substantive', 142), ('err', 142), ('Board', 142), ('explaining', 142), ('leave', 142), ('Anders', 142), ('post', 142), ('significant', 142), ('conclusion', 141), ('reasonableness', 141), ('arrest', 141), ('make', 140), ('per', 140), ('determine', 140), ('apply', 140), ('pursuant', 140), ('dismiss', 139), ('June', 138), ('payment', 138), ('burden', 138), ('separate', 138), ('services', 138), ('untimely', 138), ('January', 138), ('Corp', 138), ('health', 138), ('qualified', 137), ('analysis', 137), ('determining', 137), ('submission', 137), ('history', 137), ('involved', 136), ('contemplated', 136), ('Note', 136), ('present', 135), ('Stat', 135), ('16', 135), ('arguments', 135), ('substantively', 135), ('applied', 134), ('crime', 134), ('pay', 134), ('mailing', 134), ('Miller', 134), ('II', 133), ('P', 133), ('quotation', 133), ('F2d', 133), ('electronically', 133), ('liability', 133), ('told', 132), ('agree', 132), ('August', 132), ('therefore', 132), ('111', 132), ('copies', 132), ('thus', 132), ('duty', 132), ('probable', 132), ('improper', 132), ('DL', 132), ('March', 131), ('property', 131), ('gun', 131), ('medical', 131), ('Courthouse', 130), ('confidence', 130), ('Paper', 130), ('clerks', 129), ('violated', 129), ('Brown', 129), ('challenges', 128), ('noted', 128), ('substance', 128), ('Defendants', 128), ('Although', 128), ('Eagleton', 128), ('Room', 128), ('24329', 128), ('63102', 128), ('VOICE', 128), ('2442400', 128), ('FAX', 128), ('2442780', 128), ('wwwca8uscourtsgov', 128), ('RE', 128), ('Dear', 128), ('Counselfiled', 128), ('CMECF', 128), ('grace', 128), ('postmark', 128), ('prosefiled', 128), ('FRAP', 128), ('Enclosures', 128), ('cc', 128), ('CourtAgency', 128), ('Numbers', 128), ('affirmed', 128), ('employment', 128), ('offenses', 127), ('constitutional', 127), ('May', 127), ('statement', 127), ('Fed', 127), ('December', 127), ('reasonably', 126), ('actual', 126), ('2007', 126), ('interpretation', 126), ('weight', 126), ('report', 126), ('controlled', 125), ('account', 125), ('actions', 125), ('parents', 125), ('rather', 124), ('STRAS', 124), ('2003', 124), ('care', 124), ('DLs', 124), ('denying', 123), ('shall', 123), ('holding', 123), ('1988', 123), ('cocaine', 123), ('immunity', 123), ('Pharmacies', 123), ('either', 122), ('October', 122), ('official', 122), ('well', 121), ('BIA', 121), ('favor', 121), ('738', 121), ('Dakota', 121), ('injury', 121), ('dismissed', 120), ('reason', 120), ('challenge', 120), ('20', 120), ('Scott', 120), ('Fourth', 120), ('failure', 120), ('caused', 120), ('asked', 119), ('Ohio', 119), ('ERICKSON', 118), ('business', 118), ('2006', 118), ('provides', 118), ('C', 118), ('Second', 118), ('certain', 118), ('488', 118), ('386', 118), ('distribution', 118), ('give', 118), ('enforcement', 118), ('set', 117), ('persecution', 117), ('point', 117), ('original', 117), ('NPS', 117), ('prove', 116), ('September', 116), ('extent', 116), ('limited', 116), ('authority', 116), ('requires', 116), ('42', 116), ('included', 116), ('Smith', 115), ('application', 115), ('agreed', 115), ('She', 114), ('number', 114), ('contends', 114), ('felony', 114), ('necessary', 114), ('condition', 114), ('activity', 113), ('Under', 113), ('Department', 113), ('us', 113), ('75', 113), ('likely', 113), ('fraud', 113), ('2005', 112), ('remand', 112), ('rule', 112), ('1967', 112), ('David', 112), ('like', 112), ('sex', 112), ('However', 111), ('statements', 111), ('require', 111), ('plaintiff', 111), ('take', 111), ('least', 110), ('insufficient', 110), ('include', 110), ('If', 110), ('Appellants', 110), ('Here', 110), ('direct', 110), ('money', 109), ('specific', 109), ('concluding', 109), ('vehicle', 109), ('next', 109), ('judge', 109), ('knew', 108), ('argue', 108), ('Penson', 108), ('related', 108), ('plain', 108), ('risk', 108), ('Bankruptcy', 108), ('KELLY', 107), ('car', 107), ('phone', 107), ('arguing', 107), ('lack', 107), ('Kansas', 107), ('nonfrivolous', 107), ('reduction', 107), ('records', 106), ('paid', 106), ('Appellees', 106), ('SHEPHERD', 106), ('language', 106), ('Bad', 106), ('several', 105), ('charge', 105), ('provisions', 105), ('Finally', 105), ('regarding', 105), ('supports', 105), ('Plaintiff', 105), ('general', 105), ('Ahmed', 105), ('convictions', 104), ('Immigration', 104), ('CFR', 104), ('capacity', 104), ('There', 103), ('additional', 103), ('independently', 103), ('marks', 102), ('GRASZ', 102), ('counsels', 102), ('Robert', 102), ('Defendant', 102), ('curiam', 102), ('contracts', 102), ('insurance', 102), ('school', 102), ('November', 101), ('marijuana', 101), ('following', 101), ('internal', 101), ('citing', 101), ('subject', 101), ('admitted', 101), ('residence', 101), ('Suite', 101), ('Section', 101), ('Jr', 101), ('dispute', 101), ('allegations', 101), ('purposes', 100), ('IJ', 100), ('never', 100), ('His', 100), ('knowingly', 100), ('outside', 100), ('given', 100), ('interference', 100), ('Haynes', 100), ('BENTON', 99), ('reviewing', 99), ('element', 98), ('emphasis', 98), ('While', 98), ('Minn', 98), ('Ins', 98), ('stop', 98), ('sentences', 97), ('applies', 97), ('proof', 97), ('group', 97), ('1983', 97), ('defense', 97), ('W', 97), ('Ferguson', 97), ('confusion', 97), ('house', 96), ('acts', 96), ('Order', 96), ('left', 96), ('LOKEN', 96), ('TransAm', 96), ('Health', 96), ('PNC', 96), ('final', 95), ('place', 95), ('less', 95), ('might', 95), ('COLLOTON', 95), ('Step', 95), ('Having', 95), ('That', 95), ('Haley', 95), ('eg', 94), ('four', 94), ('stating', 94), ('raised', 94), ('expert', 94), ('Bank', 94), ('GRUENDER', 93), ('added', 93), ('similar', 93), ('attorneys', 93), ('se', 93), ('Code', 93), ('SMI', 93), ('premises', 93), ('Allegiant', 93), ('Donelson', 93), ('attorney', 92), ('special', 92), ('proper', 92), ('file', 92), ('policies', 92), ('grams', 91), ('convicted', 91), ('challenging', 91), ('loss', 91), ('Tanner', 91), ('They', 90), ('base', 90), ('course', 90), ('dismissal', 90), ('applying', 90), ('careful', 90), ('fails', 90), ('trusts', 90), ('motions', 89), ('underlying', 89), ('social', 89), ('County', 89), ('SW2d', 89), ('Even', 89), ('benefits', 89), ('XPO', 89), ('administrative', 89), ('consistent', 88), ('deny', 88), ('Thus', 88), ('noting', 88), ('pro', 88), ('voluntarily', 88), ('17', 88), ('Jones', 88), ('July', 87), ('means', 87), ('shows', 87), ('argued', 87), ('questions', 87), ('Plaintiffs', 87), ('placed', 87), ('treatment', 87), ('obligations', 87), ('theory', 87), ('force', 87), ('charged', 86), ('cmt', 86), ('otherwise', 86), ('asserts', 86), ('North', 86), ('protected', 86), ('Sentencing', 85), ('five', 85), ('identified', 85), ('trustee', 85), ('debt', 85), ('municipalities', 85), ('transmission', 85), ('gave', 84), ('verdict', 84), ('nature', 84), ('investigation', 84), ('states', 84), ('Boy', 84), ('applicable', 84), ('Police', 84), ('began', 83), ('costs', 83), ('elements', 83), ('30', 83), ('sought', 83), ('advisory', 83), ('warrant', 83), ('containing', 82), ('way', 82), ('2004', 82), ('24', 82), ('multiple', 82), ('forth', 82), ('requirement', 82), ('test', 82), ('intentional', 82), ('LundRoss', 82), ('occurred', 81), ('cleaned', 81), ('still', 81), ('KOBES', 81), ('decided', 81), ('true', 81), ('available', 81), ('others', 80), ('unless', 80), ('judicial', 80), ('weighing', 80), ('Moore', 80), ('room', 80), ('Walmart', 80), ('quantity', 79), ('reverse', 79), ('believe', 79), ('context', 79), ('James', 79), ('good', 79), ('generally', 79), ('Chapter', 79), ('pharmacy', 79), ('trade', 79), ('Davis', 79), ('DNA', 79), ('WOLLMAN', 78), ('Further', 78), ('connection', 78), ('stay', 78), ('payments', 77), ('agency', 77), ('decisions', 77), ('exercise', 77), ('violations', 77), ('adverse', 77), ('Upon', 77), ('violence', 77), ('act', 77), ('patients', 77), ('Bivens', 77), ('McReynolds', 77), ('Madison', 77), ('role', 76), ('meaning', 76), ('2002', 76), ('impose', 76), ('initial', 76), ('liable', 76), ('dangerous', 76), ('serious', 76), ('though', 76), ('Agreement', 76), ('Ackermans', 76), ('times', 75), ('572', 75), ('harm', 75), ('Northern', 75), ('23', 75), ('potential', 75), ('coverage', 75), ('specifically', 74), ('Security', 74), ('grounds', 74), ('view', 74), ('19', 74), ('downward', 74), ('benefit', 74), ('offered', 74), ('Title', 74), ('justice', 73), ('knowledge', 73), ('failing', 73), ('27', 73), ('requested', 73), ('Law', 73), ('pled', 73), ('proposed', 73), ('appellate', 73), ('custody', 73), ('back', 73), ('robbery', 73), ('26', 73), ('took', 73), ('During', 72), ('control', 72), ('anything', 72), ('recognized', 72), ('possessed', 72), ('remedy', 72), ('reduce', 72), ('continued', 72), ('Leases', 72), ('ESI', 72), ('attempted', 71), ('total', 71), ('supported', 71), ('sexual', 71), ('contrary', 71), ('using', 71), ('22', 71), ('resulting', 71), ('felon', 71), ('change', 71), ('Gross', 71), ('third', 71), ('BR', 71), ('burglary', 71), ('Student', 71), ('Nebraska', 70), ('showing', 70), ('form', 70), ('members', 70), ('allegedly', 70), ('year', 70), ('neither', 70), ('revocation', 70), ('lllllllllllllllllllllDefendants', 70), ('documents', 70), ('life', 69), ('removed', 69), ('meet', 69), ('position', 69), ('1996', 69), ('seeking', 69), ('employees', 69), ('return', 69), ('Insurance', 69), ('education', 69), ('ERISA', 69), ('demonstrate', 68), ('calculation', 68), ('allege', 68), ('product', 68), ('statutes', 68), ('private', 68), ('every', 68), ('value', 68), ('personal', 68), ('interests', 68), ('behavior', 68), ('losses', 68), ('plans', 68), ('OPTN', 68), ('beyond', 67), ('nothing', 67), ('discussing', 67), ('protect', 67), ('effect', 67), ('Douglas', 67), ('witness', 67), ('discussed', 67), ('requirements', 67), ('service', 67), ('laws', 67), ('Restatement', 67), ('status', 67), ('acted', 67), ('Meza', 67), ('autism', 67), ('permission', 66), ('common', 66), ('written', 66), ('granting', 66), ('past', 66), ('ruling', 66), ('response', 66), ('procedural', 66), ('obligation', 66), ('pornography', 66), ('weapon', 66), ('ice', 66), ('retrieval', 66), ('Cuker', 66), ('showed', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "  \n",
    "  \n",
    "# split() returns list of all the words in the string\n",
    "  \n",
    "# Pass the split_it list to instance of Counter class.\n",
    "Counter = Counter(tokens_without_sw)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter.most_common(1000)\n",
    "  \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "IbiZX-KRrJFl",
    "outputId": "561e7e0c-91ac-43be-a119-c1137ae63022"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6a7f596c-d8a1-47ff-81da-54b5071e23d1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very good day,so fast and slow,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a random thought,the meaning of this,2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maybe like this,here you go,3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a7f596c-d8a1-47ff-81da-54b5071e23d1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6a7f596c-d8a1-47ff-81da-54b5071e23d1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6a7f596c-d8a1-47ff-81da-54b5071e23d1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                   Output\n",
       "0        very good day,so fast and slow,1\n",
       "1  a random thought,the meaning of this,2\n",
       "2           maybe like this,here you go,3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "A = ['very good day', 'a random thought', 'maybe like this']\n",
    "B = ['so fast and slow', 'the meaning of this', 'here you go']\n",
    "C = [1, 2, 3]\n",
    "\n",
    "pdt_items = pd.DataFrame({'A':A,'B':B,'C':C})\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# use pd.DataFrame here to avoid your error and add your column name    \n",
    "sample = pd.DataFrame(pdt_items['A']+','+pdt_items['B']+','+pdt_items['C'].astype('str'), columns=['Output'])\n",
    "\n",
    "vectorized = cv.fit_transform(sample['Output'])\n",
    "\n",
    "sample"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS_490_DATA_UNDERSTANDING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
